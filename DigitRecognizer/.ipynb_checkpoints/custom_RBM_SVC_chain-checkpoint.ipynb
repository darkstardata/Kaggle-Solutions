{
 "metadata": {
  "name": "",
  "signature": "sha256:c5203a8c5dbf0a8688bdac18749ed7e180990d400d07174c741d01cd6fe3346a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Kaggle project: https://www.kaggle.com/c/digit-recognizer\n",
      "\n",
      "The goal in this competition is to take an image of a handwritten single digit,\n",
      "and determine what that digit is.  As the competition progresses, we will release \n",
      "tutorials which explain different machine learning algorithms and help you to get started.\n",
      "\n",
      "'''\n",
      "\n",
      "# Plot package\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# pandas and numpy\n",
      "# not so much of pandas but for read_csv which is more efficient than numpy.loadtxt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# scikit-learn classifiers and cross validation utils\n",
      "from sklearn.neural_network import BernoulliRBM\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "# Preprocessing and pipeline\n",
      "from sklearn.preprocessing import scale\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "# scikit-learn dimension reduction\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# scipy image shifts and rotations\n",
      "from scipy.ndimage import convolve\n",
      "import scipy.ndimage as nd\n",
      "\n",
      "# Model evaluation\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# Beep when finished\n",
      "import ipybell\n",
      "\n",
      "import time\n",
      "\n",
      "wdir = '/Users/andrew/Documents/data/MNIST'    \n",
      "\n",
      "\n",
      "def notify():\n",
      "    %bell -n osx time.sleep(0.5)\n",
      "    %bell -n osx time.sleep(0.5)\n",
      "    %bell -n osx time.sleep(0.5)\n",
      "    %bell -n osx time.sleep(1)\n",
      "    %bell -n osx time.sleep(0.5)\n",
      "    %bell -n osx time.sleep(0.5)\n",
      "\n",
      "    \n",
      "def nudge_dataset(X, Y):\n",
      "    \"\"\"\n",
      "    This produces a dataset 9 times bigger than the original one,\n",
      "    by moving the 8x8 images in X around by 1px to left, right, down, up, \n",
      "    up + left, up + right, down + left, down + right.\n",
      "    \"\"\"\n",
      "    direction_vectors = [\n",
      "        [[0, 1, 0],\n",
      "         [0, 0, 0],\n",
      "         [0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0],\n",
      "         [1, 0, 0],\n",
      "         [0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0],\n",
      "         [0, 0, 0],\n",
      "         [0, 1, 0]]]\n",
      "    shift = lambda x, w: convolve(x.reshape((28, 28)), mode='constant',weights=w).ravel()\n",
      "    X = np.concatenate([X] + [np.apply_along_axis(shift, 1, X, vector) for vector in direction_vectors])\n",
      "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
      "    return X, Y\n",
      "\n",
      "\n",
      "def PCAoptimize(X, Y, n_components, whiten=False):\n",
      "    \"\"\"\n",
      "    Given an array of n_components values, this program will find the optimal array value for n_components.\n",
      "    Program returns the training set with reduced components.\n",
      "    \n",
      "    \"\"\"\n",
      "    print 'Analyzing PCA components'\n",
      "    pca = PCA(whiten=whiten)\n",
      "    pca.fit(X)\n",
      "    plt.figure(1, figsize=(4, 3))\n",
      "    plt.clf()\n",
      "    plt.axes([.2, .2, .7, .7])\n",
      "    plt.plot(pca.explained_variance_, linewidth=2)\n",
      "    plt.axis('tight')\n",
      "    plt.xlabel('n_components')\n",
      "    plt.ylabel('explained_variance_')\n",
      "    print 'Finding optimal components for PCA using GridSearchCV'\n",
      "    parameters = {'n_components':n_components}\n",
      "    estimator = GridSearchCV(pca, parameters, verbose=1, n_jobs=2)\n",
      "    estimator.fit(X, Y)\n",
      "    pca_components = estimator.best_estimator_.n_components\n",
      "    print 'Plot best fit for n_components'\n",
      "    plt.axvline(pca_components,linestyle=':', label='n_components chosen')\n",
      "    plt.legend(prop=dict(size=12))\n",
      "    plt.show()\n",
      "    print \"Fitting PCA. Components: '{0}'\".format(int(pca_components))\n",
      "    pca = PCA(n_components=pca_components, whiten=True).fit(X_train)\n",
      "    print \"Reducing training set to '{0}' components\".format(int(pca_components))\n",
      "    X_train_reduced = pca.transform(X_train)\n",
      "    print 'Completed PCA optimization'\n",
      "    \n",
      "    return X_train_reduced, pca_components\n",
      "\n",
      "\n",
      "def Predict(X, Y, X_test, pca_components, C, kernel, degree, gamma):\n",
      "\n",
      "    \n",
      "    pipeline = Pipeline()\n",
      "    print \"Converting training set to matrix\"\n",
      "    X_train = np.mat(X)\n",
      "    print \"Fitting SVC with training set and parameters C='{0}', kernel ='{1}'\".format(int(C), kernel)\n",
      "    clf = SVC(C = C, kernel = kernel, degree = degree, gamma = gamma)\n",
      "    print clf.fit(X_train_reduced, Y)\n",
      "    print \"Reducing test set to '{0}' components\".format(int(pca_components))\n",
      "    X_test_reduced = pca.transform(X_test)\n",
      "    print \"Predicting numbers\"\n",
      "    predictions = clf.predict(X_test_reduced)\n",
      "    print \"Writing to file\"\n",
      "    idheader = np.arange(1,len(predictions)+1,1)\n",
      "    np.savetxt(wdir + \"/predicted_9x.csv\", zip(idheader,predictions), header = 'ImageId,Label', \n",
      "               comments = '', fmt = '%i', delimiter=\",\")\n",
      "\n",
      "    return predictions\n",
      "\n",
      "def SVCoptimize(X, Y, C, degree, gamma, test_frac, kernel='rbf'):\n",
      "    print 'Splitting training set into training and test sets'\n",
      "    X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=test_frac, random_state=42)\n",
      "    print 'Finding optimal parameters for SVC using GridSearchCV'\n",
      "    svc = SVC(kernel=kernel)\n",
      "    parameters = {'C':C,'degree':degree,'kernel':kernel,'gamma':gamma}\n",
      "    estimator = GridSearchCV(svc, parameters, verbose=1, scoring='accuracy', n_jobs=2)\n",
      "    estimator.fit(X_train[:100000], y_train[:100000])\n",
      "    # Best fit parameters\n",
      "    C = estimator.best_estimator_.C\n",
      "    kernel = estimator.best_estimator_.kernel\n",
      "    degree = estimator.best_estimator_.degree\n",
      "    gamma = estimator.best_estimator_.gamma\n",
      "    # Print results\n",
      "    print '\\n\\n Best Estimators'\n",
      "    print '-----------------'\n",
      "    print '           C = ' + str(C)\n",
      "    print '      kernel = ' + str(kernel)\n",
      "    print '      degree = ' + str(degree)\n",
      "    print '       gamma = ' + str(gamma)\n",
      "    print '-----------------'\n",
      "    print '  best score = ' + str(estimator.best_score_)\n",
      "    print '\\n \\n'\n",
      "    predictions = estimator.predict(X_test)\n",
      "    print classification_report(y_test, predictions)\n",
      "    print 'Completed SVC optimization'\n",
      "    \n",
      "    return predictions, C, kernel, degree, gamma\n",
      "\n",
      "\n",
      "def RMBoptimize():\n",
      "    \n",
      "    return\n",
      "\n",
      "\n",
      "def Predict_RBM_SVC(X_train, X_test, Y_train, Y_test):\n",
      "    # Models we will use\n",
      "    rbm = BernoulliRBM(random_state=0, verbose=True)\n",
      "    svc = SVC(verbose=True)\n",
      "\n",
      "    classifier = Pipeline(steps=[('rbm', rbm), ('svc', svc)])\n",
      "\n",
      "    \n",
      "    # Optimizing model using GridSearchCV\n",
      "    \n",
      "    \n",
      "    ###############################################################################\n",
      "    # Training\n",
      "\n",
      "    # Hyper-parameters. These were set by cross-validation,\n",
      "    # using a GridSearchCV. Here we are not performing cross-validation to\n",
      "    # save time.\n",
      "    rbm.learning_rate = 0.06\n",
      "    rbm.n_iter = 20\n",
      "    # More components tend to give better prediction performance, but larger\n",
      "    # fitting time\n",
      "    rbm.n_components = 100\n",
      "    logistic.C = 6000.0\n",
      "\n",
      "    # Training RBM-Logistic Pipeline\n",
      "    classifier.fit(X_train, Y_train)\n",
      "\n",
      "    # Training Logistic regression\n",
      "    logistic_classifier = linear_model.LogisticRegression(C=100.0)\n",
      "    logistic_classifier.fit(X_train, Y_train)\n",
      "\n",
      "    ###############################################################################\n",
      "    # Evaluation\n",
      "\n",
      "    print()\n",
      "    print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
      "        metrics.classification_report(\n",
      "            Y_test,\n",
      "            classifier.predict(X_test))))\n",
      "\n",
      "    print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
      "        metrics.classification_report(\n",
      "            Y_test,\n",
      "            logistic_classifier.predict(X_test))))\n",
      "\n",
      "    ###############################################################################\n",
      "    # Plotting\n",
      "\n",
      "    plt.figure(figsize=(4.2, 4))\n",
      "    for i, comp in enumerate(rbm.components_):\n",
      "        plt.subplot(10, 10, i + 1)\n",
      "        plt.imshow(comp.reshape((28, 28)), cmap=plt.cm.gray_r,\n",
      "                   interpolation='nearest')\n",
      "        plt.xticks(())\n",
      "        plt.yticks(())\n",
      "    plt.suptitle('100 components extracted by RBM', fontsize=16)\n",
      "    plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
      "\n",
      "    plt.show()\n",
      "    notify()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load data and save header\n",
      "train_csv = pd.read_csv(wdir + '/train.csv')\n",
      "test_csv = pd.read_csv(wdir + '/test.csv')\n",
      "hdr = list(train_csv)\n",
      "\n",
      "# Convert from DataFrames to Numpy matrices\n",
      "y_train = train_csv['label'].values\n",
      "X_train = train_csv.loc[:,'pixel0':].values\n",
      "X_test = test_csv.loc[:,'pixel0':].values\n",
      "\n",
      "# Increase training set by 5 fold\n",
      "X_train, y_train = nudge_dataset(X_train, y_train)\n",
      "print 'Nudging complete'\n",
      "\n",
      "\n",
      "train_csv = pd.read_csv(wdir + '/train_5x.csv')\n",
      "X = np.asarray(train_csv.loc[:,'pixel0':].values, dtype = 'float32')\n",
      "#X = scale(X)\n",
      "Y = np.asarray(train_csv['label'].values, dtype = 'float32')\n",
      "\n",
      "print X.shape\n",
      "print Y.shape\n",
      "X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
      "                                                    test_size=0.2,\n",
      "                                                    random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}